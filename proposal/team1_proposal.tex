\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{listings}
\lstset{
  frame=single,
  language=python,
  basicstyle=\small,
}

\makeatletter
\def\lst@makecaption{%
  \def\@captype{table}%
  \@makecaption
}
\makeatother

\begin{document}

\title{CS525 - Team 1 Project 4 Proposal}

\author{
\IEEEauthorblockN{Alexander Moore}
\IEEEauthorblockA{\textit{Data Science} \\
\textit{Worcester Polytechnic Institute}\\
Worcester, United States \\
ammoore@wpi.edu}

\and

\IEEEauthorblockN{Brian Lewandowski}
\IEEEauthorblockA{\textit{Computer Science} \\
\textit{Worcester Polytechnic Institute}\\
Worcester, United States \\
balewandowski@wpi.edu}

\and

\IEEEauthorblockN{Jannik Haas}
\IEEEauthorblockA{\textit{Data Science} \\
\textit{Worcester Polytechnic Institute}\\
Worcester, United States \\
jbhaas@wpi.edu}

\and

\IEEEauthorblockN{Quincy Hershey}
\IEEEauthorblockA{\textit{Data Science} \\
\textit{Worcester Polytechnic Institute}\\
Worcester, United States \\
qbhershey@wpi.edu}

\and

\IEEEauthorblockN{Scott Tang}
\IEEEauthorblockA{\textit{Data Science} \\
\textit{Worcester Polytechnic Institute}\\
Worcester, United States \\
stang3@wpi.edu}
}

\maketitle

\begin{abstract}
    Understanding which models succeed at which tasks and why is a foundational experience in reinforcement learning.
    Following a baseline of techniques and best practices found in the literature this project will show a comparison of multiple reinforcement learning techniques applied in a few common environments.
    In particular, the aim of this project is to implement several Q-learning and policy learning algorithms and compare the results across diverse tasks.
    In addition, the results attained by this work will be compared to those already reported in established works. This work will serve as an exploration of baseline results for diverse reinforcement learning algorithms, in order to compare and contrast the performance of models on diverse tasks. Potential conclusions of this work could include suggestions for appropriate models given the task, heuristic hyperparameters which work for many models, and potential evidence that some models always outperform others.
\end{abstract}

\begin{IEEEkeywords}
Q-Learning, Policy Learning, DQN, Policy Gradient, Reinforcement Learning
\end{IEEEkeywords}

\section{Introduction}
Project 3 explored how to construct a Deep Q Network (DQN) by following the classic DQN algorithm outlined originally in \cite{DQNOriginalPaper}.
Building off of this recent experience this project proposes that deep learning techniques applied to the area of policy learning are implemented and compared to the results of DQNs.

In addition, this project proposes to use \cite{nichol2018retro} as a baseline for the environment and games to use for comparison of techniques as it is considered more difficult than the Atari 2600 games contained in the Arcade Learning Environment \cite{Bellemare_2013}.
Using this environment multiple agents will be trained using policy learning techniques such as vanilla policy gradient, proximal policy optimization (PPO), and trust region policy optimization (TRPO), with the final cast of models to be determined based on compute-time.

The results of these agents will then be compared to trained Q-learning agents implemented as a DQN, Double DQN (DDQN), and Dueling DDQN (for example); while using the results to assess the strengths and weaknesses of each. By comparing multiple agents across multiple games we will create a case for the superiority or inferiority of some algorithms to others, or find that there is no one algorithm which consistently outperforms others.

The remainder of this proposal is as follows.
Section II provides background information regarding the methods planned to be implemented.
Section III outlines the specific work planned for this project.
Section IV proposes a timeline for the completion of project milestones.
Section VI indicates the expected deliverables to be generated and provided as part of this work.
Finally, the proposal concludes in section VII.

\section{Background Information}
Our goals may change for implementation. For example, we may focus on a smaller amount of models and instead compare tweaks on these algorithms to demonstrate and quantify the improvement they offer.
\subsection{Deep Q Learning}
Deep Q learning uses a deep neural network to learn coefficients $\omega$ such that the network's value function evaluates $(state, action)$ pairs improving the model's task reward. For some of our environments this will be a convolution over a screen space, and for some we might extract some derived features about the game for the model to use as a state representation.

\subsection{Double Deep Q Learning}
Double deep Q networks address the maximization bias problem from Deep q learning by instead letting two Q functions randomly select the action and update the corresponding Q function. This process inhibits bias and might outperform DQN on some or all tasks.

\subsection{Dueling Deep Q Learning}
Dueling DQN separates the Q-learning process into two functions, the sum of the state and state-action estimator models. This approach ideally learns how to relatively value states. For this reason this different approach will be interseting to analyze on tasks where the actions might not always directly affect the environment.

\section{Planned Work}

\subsection{Background Research}
Research is planned to be performed to understand all of the techniques to be exercised during this project.
In addition, the literature will be surveyed to identify any recent developments or improvements related to the methods being employed.
While doing this, we also plan to obtain available benchmarks for techniques such as those found in \cite{DQNOriginalPaper}, \cite{NatureDeepLearning}, and \cite{bhonker2017playing}.

\subsection{Environment Setup}
This part of the project involves ensuring that all team members have the ability to develop and assess the algorithms under study.
Some of the key areas include ensuring a common (or compatible) software environment is available including the ability to run \cite{nichol2018retro} either locally or on a WPI asset such as the Ace cluster.

In addition, a simple framework or skeleton code should be utilized such that all algorithm implementations are structurally similar.
We plan to take advantage of the provided framework from Project 3 to reduce the amount of work necessary in this area.

\subsection{Algorithm Implementations}
The algorithm implementations involve taking the actual theoretical algorithms and implementing them in Python code using common libraries such as PyTorch and NumPy.
The goal of these implementations is to be true to the algorithms and any notable improvements that have become commonplace in practice.

While deviations to these algorithms may become necessary for practical reasons, they will be noted in the final report provided.

\subsection{Results Analysis}
During the analysis of the results each model will be compared using common metrics such reward per episode and the number of episodes necessary for convergence.
In addition, commentary will be provided of any interesting results or findings from the experiments that do not fit into the objective metrics being measured.

\subsection{Paper Preparation}
A roughly 10 page paper will be put together to conclude this project and communicate its findings.
This paper will include any background information necessary for the project, a description of methodologies used, results, and an analysis of the results.

\subsection{Presentation Preparation}
The work of this project will be summarized in a presentation to be delivered on the poster day for the project.
If any work is found to be an interesting demo it would be displayed during the presentation time. Potential presentation results include:
\begin{itemize}
    \item Expected score in testing after a fixed training time in seconds or iterations for each game
    \item Necessary training time in seconds or iterations to achieve a fixed score for each game
    \item Comparison of learning rates of models for each game
    \item Meta-comparison of games, their action spaces, and relevant challenges and tasks
\end{itemize}

\section{Tools and Environment}

\subsection{Implementation Language}
This project will be using the Python language of at least version 3.7.
In addition to the standard libraries, it is expected that the following additional libraries will be utilized and standardized across group developement machines:
 
\begin{itemize}
    \item Gym Retro \cite{nichol2018retro}
    \item PyTorch
    \item NumPy
    \item Matplotlib
    \item Pandas
\end{itemize}

In addition, the framework from Project 3 will be utilized as a baseline for implementing the various algorithms being researched. Each algorithm can be fit in this framework to facilitate plug-and-train pipelines for each different model and game.

\subsection{Reinforcement Learning Environment}
This project plans to use an existing reinforcement learning environment while assessing the various algorithm implementations.
In particular, the Retro Learning Environment \cite{nichol2018retro} will be utilized for its large selection of games with high levels of complexity.
In addition, it maintains the same API as the environment used in Project 3 making it close to a drop-in replacement for the existing Project 3 framework.

The initial plan is to focus on the games explored in \cite{bhonker2017playing} as there is a baseline result for several algorithms and a known level of complexity.
Some of the options to be explored include F-Zero, Gradius 3, Mortal Kombat, and Wolfenstein.
Note that the choice of game is subject to change depending on availability of published results and practical reasons given the time frame for the project.
These games represent diversity in action spaces, reward sparsity, action-memory importance, and state dimensionality.

\subsection{Compute Resources}
The plan for compute resources is to use a combination of local machines as well as the Ace cluster at WPI.
Several team members have access to personal machines with GPUs while other team members have access to the Ace cluster.
These resources combined should provide sufficient computing resources to be successful on this project.
Dispatching batches of jobs to the Turing machine with GPU requirements can also expedite and parallelize long training times.

\section{Schedule}
This section outlines the schedule for this project on a weekly basis with the main tasks enumerated within the week during which they are expected to complete.
It should be noted that this is a tentative schedule to serve as a guideline and some items may be adjusted as needed through the course of the project.

\subsection{Week Ending 11/13}
\begin{enumerate}
    \item Refine Project Idea
    \item Develop Project Proposal
    \item Submit Project Proposal
\end{enumerate}

\subsection{Week Ending 11/20}
\begin{enumerate}
    \item Set up environment infrastructure
    \item Research and understand algorithms
    \item Begin implementing algorithms
\end{enumerate}

\subsection{Week Ending 11/27}
\begin{enumerate}
    \item Continue algorithm implementation
    \item Begin algorithm execution
    \item Refine model hyperparameters as needed
    \item Collect data for analysis
    \item Prepare and deliver project progress report
\end{enumerate}

\subsection{Week Ending 12/04}
\begin{enumerate}
    \item Analyze and compare model performance
    \item Begin writing report
    \item Begin working on presentation
\end{enumerate}

\subsection{Week Ending 12/11}
\begin{enumerate}
    \item Complete report
    \item Complete presentation
    \item Dry run presenation
    \item Submit artifacts
    \item Present findings during poster session
\end{enumerate}

\section{Deliverables}

\subsection{Progress Report}
A progress report is planned to be put together and delivered by 11/26 in accordance with the project requirements.
This report is expected to provide a clear picture of progress made on the project to date and the plan for the project over the remainder of the semester.
In addition, any deviations from this proposal will be indicated as necessary in the progress report. At this time any concerns about compute and development time can be brought up to re-evaluate the proposed methods and goals of the project to conclude within the project time.

\subsection{Final Report}
The final report provided as part of this project will include the necessary background information required to understand the methods used.
In addition, our findings on $N*K$ trained models where N is our number of agents and K is our number of games will be included.
The analysis of our findings will include theory on why some models performed better or worse given the game tasks, appropriate model choices for different tasks, and a discussion on our findings of what succeeded and failed for developing diverse models over diverse tasks.
In addition, a description of our methodology will be provided such that others could use our workflow to do the same experiments.

\subsection{Code and Model Artifacts}
All code and artifacts needed to duplicate this project will be provided as part of the deliverables for this project.
In addition, all trained models used in reported performance measures will be saved and provided.

\subsection{Presentation}
The presentation delivered as part of this project will provide an overview of the information covered in the final report.
This includes brief background information and a focus on the comparative results received.
If applicable, any live demos will be rendered and performed during this portion of the project.

\section{Conclusion}
This proposal has provided a detailed summary of the planned project including its overall objectives, an outline of the methodology proposed, as well as a detailed schedule.
We welcome any feedback and look forward to exploring this area of reinforcement learning over the next few weeks.

\bibliography{citations.bib}{}
\bibliographystyle{plain}

\vspace{12pt}

\end{document}
